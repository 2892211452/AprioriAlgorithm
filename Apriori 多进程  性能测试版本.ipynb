{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# frozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Config import *\n",
    "\n",
    "\n",
    "# 倒入设置参数和加载数据集函数\n",
    "from Config import *\n",
    "\n",
    "\n",
    "\n",
    "# 返回只有单个元素的候选集\n",
    "def createC1(dataSet):\n",
    "    '''\n",
    "        构建初始候选项集的列表，即所有候选项集只包含一个元素，\n",
    "        C1是大小为1的所有候选项集的集合\n",
    "    '''\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if [item] not in C1:\n",
    "                C1.append([item])\n",
    "    C1.sort()\n",
    "    # return map( frozenset, C1 )\n",
    "    # return [var for var in map(frozenset,C1)]\n",
    "    return [frozenset(var) for var in C1]\n",
    "\n",
    "\n",
    "def scanD(D, Ck, minSupport):\n",
    "    '''\n",
    "        计算Ck中的项集在数据集合D(记录或者transactions)中的支持度,\n",
    "        返回满足最小支持度的项集的集合，和所有项集支持度信息的字典。\n",
    "    '''\n",
    "    print(len(Ck))\n",
    "    ssCnt = {}\n",
    "    for tid in D:  # 对于每一条transaction\n",
    "        for can in Ck:  # 对于每一个候选项集can，检查是否是transaction的一部分 # 即该候选can是否得到transaction的支持\n",
    "            flag = True\n",
    "            for i in can:\n",
    "                if i not in tid:\n",
    "                    flag = False\n",
    "                    \n",
    "            if flag:\n",
    "                ssCnt[can] = ssCnt.get(can, 0) + 1\n",
    "                \n",
    "#             if can.issubset(tid):\n",
    "#                 ssCnt[can] = ssCnt.get(can, 0) + 1\n",
    "    numItems = float(len(D))\n",
    "    # print(\"ssCnt is\",ssCnt)\n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key] / numItems  # 每个项集的支持度\n",
    "        if support >= minSupport:  # 将满足最小支持度的项集，加入retList\n",
    "            retList.insert(0, key)\n",
    "        supportData[key] = support  # 汇总支持度数据\n",
    "    return retList, supportData\n",
    "\n",
    "\n",
    "def aprioriGen(Lk, k):  # Aprior算法\n",
    "    '''\n",
    "        由初始候选项集的集合Lk生成新的生成候选项集，\n",
    "        k表示生成的新项集中所含有的元素个数\n",
    "        注意其生成的过程中，首选对每个项集按元素排序，然后每次比较两个项集，只有在k-1项相同时才将这两项合并。这样做是因为函数并非要两两合并各个集合，那样生成的集合并非都是k+1项的。在限制项数为k+1的前提下，只有在前k-1项相同、最后一项不相同的情况下合并才为所需要的新候选项集。\n",
    "    '''\n",
    "    retList = set()\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i + 1, lenLk):\n",
    "            \n",
    "            L1 = Lk[i]\n",
    "            L2 = Lk[j]\n",
    "            cnt =0\n",
    "            for m in L1:\n",
    "                if m in L2:\n",
    "                    cnt+=1\n",
    "            if cnt == k-2:\n",
    "                retList.add(Lk[i] | Lk[j])\n",
    "    return retList\n",
    "\n",
    "\n",
    "def apriori(dataSet, minSupport=0.5):\n",
    "    \"\"\"\n",
    "    该函数为Apriori算法的主函数，按照前述伪代码的逻辑执行。Ck表示项数为k的候选项集，最初的C1通过createC1()函数生成。Lk表示项数为k的频繁项集，supK为其支持度，Lk和supK由scanD()函数通过Ck计算而来。\n",
    "    :param dataSet:\n",
    "    :param minSupport:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    C1 = createC1(\n",
    "        dataSet)  # 构建初始候选项集C1  [frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]\n",
    "\n",
    "    D = [set(var) for var in dataSet]  # 集合化数据集\n",
    "    L1, suppData = scanD(D, C1, minSupport)  # 构建初始的频繁项集，即所有项集只有一个元素\n",
    "    L = [L1]  # 最初的L1中的每个项集含有一个元素，新生成的\n",
    "    # print()\n",
    "    k = 2  # 项集应该含有2个元素，所以 k=2\n",
    "\n",
    "    while (len(L[k - 2]) > 0):\n",
    "        print(\"iter is \", k)\n",
    "        t = time.time()\n",
    "        Ck = aprioriGen(L[k - 2], k)\n",
    "        print(f'gen coast:{time.time() - t:.8f}s')\n",
    "        \n",
    "        t = time.time()\n",
    "        Lk, supK = scanD(D, Ck, minSupport) # 筛选最小支持度的频繁项集\n",
    "        print(f'scan coast:{time.time() - t:.8f}s')\n",
    "        # print(\"iter is \")\n",
    "        # print(Ck)\n",
    "        # print(Lk)\n",
    "        # print()\n",
    "        suppData.update(supK)  # 将新的项集的支持度数据加入原来的总支持度字典中\n",
    "        L.append(Lk)  # 将符合最小支持度要求的项集加入L\n",
    "        k += 1  # 新生成的项集中的元素个数应不断增加\n",
    "    return L, suppData  # 返回所有满足条件的频繁项集的列表，和所有候选项集的支持度信息\n",
    "\n",
    "\n",
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):  # 规则生成与评价\n",
    "    '''\n",
    "        计算规则的可信度，返回满足最小可信度的规则。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中所有的元素\n",
    "        supportData(dic):频繁项集中所有元素的支持度\n",
    "        brl(tuple):满足可信度条件的关联规则\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    prunedH = []\n",
    "    for conseq in H:\n",
    "        conf = supportData[freqSet] / supportData[freqSet - conseq]\n",
    "        if conf >= minConf:\n",
    "            print(freqSet - conseq, '-->', conseq, 'conf:', conf)\n",
    "            brl.append((freqSet - conseq, conseq, conf))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH\n",
    "\n",
    "\n",
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    '''\n",
    "        对频繁项集中元素超过2的项集进行合并。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中的所有元素，即可以出现在规则右部的元素\n",
    "        supportData(dict):所有项集的支持度信息\n",
    "        brl(tuple):生成的规则\n",
    "    '''\n",
    "    m = len(H[0])\n",
    "    if len(freqSet) > m + 1:  # 查看频繁项集是否足够大，以到于移除大小为 m的子集，否则继续生成m+1大小的频繁项集\n",
    "        Hmp1 = aprioriGen(H, m + 1)\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)  # 对于新生成的m+1大小的频繁项集，计算新生成的关联规则的右则的集合\n",
    "        if len(Hmp1) > 1:  # 如果不止一条规则满足要求（新生成的关联规则的右则的集合的大小大于1），进一步递归合并，\n",
    "            # 这样做的结果就是会有“[1|多]->多”(右边只会是“多”，因为合并的本质是频繁子项集变大，\n",
    "            # 而calcConf函数的关联结果的右侧就是频繁子项集）的关联结果\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n",
    "\n",
    "\n",
    "def generateRules(L, supportData, minConf=0.7):\n",
    "    '''\n",
    "        根据频繁项集和最小可信度生成规则。\n",
    "        L(list):存储频繁项集\n",
    "        supportData(dict):存储着所有项集（不仅仅是频繁项集）的支持度\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    bigRuleList = []\n",
    "    for i in range(1, len(L)):\n",
    "        for freqSet in L[i]:  # 对于每一个频繁项集的集合freqSet\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            if i > 1:  # 如果频繁项集中的元素个数大于2，需要进一步合并，这样做的结果就是会有“[1|多]->多”(右边只会是“多”，\n",
    "                # 因为合并的本质是频繁子项集变大，而calcConf函数的关联结果的右侧就是频繁子项集），的关联结果\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "\n",
    "    sorted(bigRuleList)\n",
    "    return bigRuleList\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lwl/Documents/GitHub/AprioriAlgorithm/Config/__init__.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ans = np.array(ans)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat = loadTest()  # 导入数据集\n",
    "\n",
    "myDat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "iter is  2\n",
      "gen coast:0.00062609s\n",
      "528\n",
      "scan coast:0.01784515s\n",
      "iter is  3\n",
      "gen coast:0.02405310s\n",
      "4204\n",
      "scan coast:0.10617709s\n",
      "iter is  4\n",
      "gen coast:0.54108167s\n",
      "20145\n",
      "scan coast:0.72195506s\n",
      "iter is  5\n",
      "gen coast:8.97044992s\n",
      "65366\n",
      "scan coast:3.34417105s\n",
      "iter is  6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lg/mhlpcshn2gg4_xl2dzh3t0gm0000gn/T/ipykernel_71406/2334743786.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.22\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 选择频繁项集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(u\"频繁项集L：\", suppData)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(u\"所有候选项集的支持度信息：\", suppData)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lg/mhlpcshn2gg4_xl2dzh3t0gm0000gn/T/ipykernel_71406/683278826.py\u001b[0m in \u001b[0;36mapriori\u001b[0;34m(dataSet, minSupport)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iter is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mCk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maprioriGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'gen coast:{time.time() - t:.8f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lg/mhlpcshn2gg4_xl2dzh3t0gm0000gn/T/ipykernel_71406/683278826.py\u001b[0m in \u001b[0;36maprioriGen\u001b[0;34m(Lk, k)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mL2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mL2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mcnt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "L, suppData = apriori(myDat, para['minSupport'])  # 选择频繁项集\n",
    "# print(u\"频繁项集L：\", suppData)\n",
    "# print(u\"所有候选项集的支持度信息：\", suppData)\n",
    "print(f'花费的时间为:{time.time() - t:.8f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "148\n",
      "415\n",
      "664\n",
      "658\n",
      "413\n",
      "161\n",
      "37\n",
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in L:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Apriori 多进程 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.7.13', ray_version='1.12.0', ray_commit='f18fc31c7562990955556899090f8e8656b48d2d', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-05-02_00-35-45_254522_4401/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-05-02_00-35-45_254522_4401/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-05-02_00-35-45_254522_4401', 'metrics_export_port': 62852, 'gcs_address': '127.0.0.1:63342', 'address': '127.0.0.1:63342', 'node_id': '60993580a92ec439cb64cedc9c1226e77edeec723fd0b88ce7c40ddb'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Config import *\n",
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 返回只有单个元素的候选集\n",
    "def createC1(dataSet):\n",
    "    '''\n",
    "        构建初始候选项集的列表，即所有候选项集只包含一个元素，\n",
    "        C1是大小为1的所有候选项集的集合\n",
    "    '''\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if [item] not in C1:\n",
    "                C1.append([item])\n",
    "    C1.sort()\n",
    "    # return map( frozenset, C1 )\n",
    "    # return [var for var in map(frozenset,C1)]\n",
    "    return [frozenset(var) for var in C1]\n",
    "\n",
    "\n",
    "def scanD(D, Ck, minSupport):\n",
    "    '''\n",
    "        计算Ck中的项集在数据集合D(记录或者transactions)中的支持度,\n",
    "        返回满足最小支持度的项集的集合，和所有项集支持度信息的字典。\n",
    "    '''\n",
    "    print(len(Ck))\n",
    "    ssCnt = {}\n",
    "    for tid in D:  # 对于每一条transaction\n",
    "        for can in Ck:  # 对于每一个候选项集can，检查是否是transaction的一部分 # 即该候选can是否得到transaction的支持\n",
    "            flag = True\n",
    "            for i in can:\n",
    "                if i not in tid:\n",
    "                    flag = False\n",
    "                    \n",
    "            if flag:\n",
    "                ssCnt[can] = ssCnt.get(can, 0) + 1\n",
    "\n",
    "    numItems = float(len(D))\n",
    "    # print(\"ssCnt is\",ssCnt)\n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key] / numItems  # 每个项集的支持度\n",
    "        if support >= minSupport:  # 将满足最小支持度的项集，加入retList\n",
    "            retList.insert(0, key)\n",
    "        supportData[key] = support  # 汇总支持度数据\n",
    "    return retList, supportData\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "myDat = loadTest()  # 导入数据集\n",
    "\n",
    "Ds = [copy.deepcopy(myDat) for i in range(10)]\n",
    "def total_scan(D, Ck, minSupport):\n",
    "    global Ds\n",
    "    # 分为10核\n",
    "    Cs = []\n",
    "    Ck =list(Ck)\n",
    "    print(len(Ck))\n",
    "    single_size = int(len(Ck)/10)\n",
    "    for i in range(10):\n",
    "        Cs.append(Ck[i* single_size : (i+1)*single_size])\n",
    "    Cs.append(Ck[10 * single_size: -1])\n",
    "    \n",
    "    t = time.time()\n",
    "    ans = [single_scan.remote(Ds[i], Cs[i], minSupport) for i in range(10)]\n",
    "    \n",
    "    \n",
    "    ans = ray.get(ans)\n",
    "    print(f'scan coast:{time.time() - t:.8f}s')\n",
    "\n",
    "    tmp = {}\n",
    "    for i in ans:\n",
    "        for j in i:\n",
    "            if i[j] > minSupport:\n",
    "                tmp[j] = i[j]\n",
    "#     print(tmp)\n",
    "    ret_list = [i for i in tmp]\n",
    "    return ret_list, tmp\n",
    "    \n",
    "\n",
    "@ray.remote\n",
    "def single_scan(D, Ck, minS):\n",
    "    r, c = scanD(D, Ck,minS)\n",
    "    return c\n",
    "\n",
    "\n",
    "def aprioriGen(Lk, k):  # Aprior算法\n",
    "    '''\n",
    "        由初始候选项集的集合Lk生成新的生成候选项集，\n",
    "        k表示生成的新项集中所含有的元素个数\n",
    "        注意其生成的过程中，首选对每个项集按元素排序，然后每次比较两个项集，只有在k-1项相同时才将这两项合并。这样做是因为函数并非要两两合并各个集合，那样生成的集合并非都是k+1项的。在限制项数为k+1的前提下，只有在前k-1项相同、最后一项不相同的情况下合并才为所需要的新候选项集。\n",
    "    '''\n",
    "    retList = set()\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i + 1, lenLk):\n",
    "            \n",
    "            L1 = Lk[i]\n",
    "            L2 = Lk[j]\n",
    "            cnt =0\n",
    "            for m in L1:\n",
    "                if m in L2:\n",
    "                    cnt+=1\n",
    "            if cnt == k-2:\n",
    "                retList.add(Lk[i] | Lk[j])\n",
    "    return retList\n",
    "\n",
    "\n",
    "def apriori(dataSet, minSupport=0.5):\n",
    "    \"\"\"\n",
    "    该函数为Apriori算法的主函数，按照前述伪代码的逻辑执行。Ck表示项数为k的候选项集，最初的C1通过createC1()函数生成。Lk表示项数为k的频繁项集，supK为其支持度，Lk和supK由scanD()函数通过Ck计算而来。\n",
    "    :param dataSet:\n",
    "    :param minSupport:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    C1 = createC1(\n",
    "        dataSet)  # 构建初始候选项集C1  [frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]\n",
    "\n",
    "    D = [set(var) for var in dataSet]  # 集合化数据集\n",
    "    L1, suppData = scanD(D, C1, minSupport)  # 构建初始的频繁项集，即所有项集只有一个元素\n",
    "    L = [L1]  # 最初的L1中的每个项集含有一个元素，新生成的\n",
    "    # print()\n",
    "    k = 2  # 项集应该含有2个元素，所以 k=2\n",
    "\n",
    "    while (len(L[k - 2]) > 0):\n",
    "        print(\"iter is \", k)\n",
    "        t = time.time()\n",
    "        Ck = aprioriGen(L[k - 2], k)\n",
    "        print(f'gen coast:{time.time() - t:.8f}s')\n",
    "        \n",
    "        Lk, supK = total_scan(D, Ck, minSupport) # 筛选最小支持度的频繁项集\n",
    "        # print(\"iter is \")\n",
    "        # print(Ck)\n",
    "        # print(Lk)\n",
    "        # print()\n",
    "        suppData.update(supK)  # 将新的项集的支持度数据加入原来的总支持度字典中\n",
    "        L.append(Lk)  # 将符合最小支持度要求的项集加入L\n",
    "        k += 1  # 新生成的项集中的元素个数应不断增加\n",
    "    return L, suppData  # 返回所有满足条件的频繁项集的列表，和所有候选项集的支持度信息\n",
    "\n",
    "\n",
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):  # 规则生成与评价\n",
    "    '''\n",
    "        计算规则的可信度，返回满足最小可信度的规则。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中所有的元素\n",
    "        supportData(dic):频繁项集中所有元素的支持度\n",
    "        brl(tuple):满足可信度条件的关联规则\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    prunedH = []\n",
    "    for conseq in H:\n",
    "        conf = supportData[freqSet] / supportData[freqSet - conseq]\n",
    "        if conf >= minConf:\n",
    "            print(freqSet - conseq, '-->', conseq, 'conf:', conf)\n",
    "            brl.append((freqSet - conseq, conseq, conf))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH\n",
    "\n",
    "\n",
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    '''\n",
    "        对频繁项集中元素超过2的项集进行合并。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中的所有元素，即可以出现在规则右部的元素\n",
    "        supportData(dict):所有项集的支持度信息\n",
    "        brl(tuple):生成的规则\n",
    "    '''\n",
    "    m = len(H[0])\n",
    "    if len(freqSet) > m + 1:  # 查看频繁项集是否足够大，以到于移除大小为 m的子集，否则继续生成m+1大小的频繁项集\n",
    "        Hmp1 = aprioriGen(H, m + 1)\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)  # 对于新生成的m+1大小的频繁项集，计算新生成的关联规则的右则的集合\n",
    "        if len(Hmp1) > 1:  # 如果不止一条规则满足要求（新生成的关联规则的右则的集合的大小大于1），进一步递归合并，\n",
    "            # 这样做的结果就是会有“[1|多]->多”(右边只会是“多”，因为合并的本质是频繁子项集变大，\n",
    "            # 而calcConf函数的关联结果的右侧就是频繁子项集）的关联结果\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n",
    "\n",
    "\n",
    "def generateRules(L, supportData, minConf=0.7):\n",
    "    '''\n",
    "        根据频繁项集和最小可信度生成规则。\n",
    "        L(list):存储频繁项集\n",
    "        supportData(dict):存储着所有项集（不仅仅是频繁项集）的支持度\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    bigRuleList = []\n",
    "    for i in range(1, len(L)):\n",
    "        for freqSet in L[i]:  # 对于每一个频繁项集的集合freqSet\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            if i > 1:  # 如果频繁项集中的元素个数大于2，需要进一步合并，这样做的结果就是会有“[1|多]->多”(右边只会是“多”，\n",
    "                # 因为合并的本质是频繁子项集变大，而calcConf函数的关联结果的右侧就是频繁子项集），的关联结果\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "\n",
    "    sorted(bigRuleList)\n",
    "    return bigRuleList\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8123, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat = loadTest()  # 导入数据集\n",
    "myDat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "iter is  2\n",
      "gen coast:0.00034595s\n",
      "300\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4424)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4430)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4425)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4432)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4427)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4433)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4428)\u001b[0m 30\n",
      "scan coast:1.68275499s\n",
      "iter is  3\n",
      "gen coast:0.00255108s\n",
      "1202\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 120\n",
      "\u001b[2m\u001b[36m(single_scan pid=4430)\u001b[0m 120\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 120\n",
      "\u001b[2m\u001b[36m(single_scan pid=4425)\u001b[0m 120\n",
      "\u001b[2m\u001b[36m(single_scan pid=4432)\u001b[0m 120\n",
      "\u001b[2m\u001b[36m(single_scan pid=4427)\u001b[0m 120\n",
      "\u001b[2m\u001b[36m(single_scan pid=4433)\u001b[0m 120\n",
      "\u001b[2m\u001b[36m(single_scan pid=4428)\u001b[0m 120\n",
      "\u001b[2m\u001b[36m(single_scan pid=4424)\u001b[0m 120\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 120\n",
      "scan coast:9.77492213s\n",
      "iter is  4\n",
      "gen coast:0.02339005s\n",
      "2456\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 245\n",
      "\u001b[2m\u001b[36m(single_scan pid=4424)\u001b[0m 245\n",
      "\u001b[2m\u001b[36m(single_scan pid=4430)\u001b[0m 245\n",
      "\u001b[2m\u001b[36m(single_scan pid=4432)\u001b[0m 245\n",
      "\u001b[2m\u001b[36m(single_scan pid=4428)\u001b[0m 245\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 245\n",
      "\u001b[2m\u001b[36m(single_scan pid=4425)\u001b[0m 245\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 245\n",
      "\u001b[2m\u001b[36m(single_scan pid=4427)\u001b[0m 245\n",
      "\u001b[2m\u001b[36m(single_scan pid=4433)\u001b[0m 245\n",
      "scan coast:26.40104032s\n",
      "iter is  5\n",
      "gen coast:0.07340288s\n",
      "2795\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 279\n",
      "\u001b[2m\u001b[36m(single_scan pid=4424)\u001b[0m 279\n",
      "\u001b[2m\u001b[36m(single_scan pid=4430)\u001b[0m 279\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 279\n",
      "\u001b[2m\u001b[36m(single_scan pid=4425)\u001b[0m 279\n",
      "\u001b[2m\u001b[36m(single_scan pid=4432)\u001b[0m 279\n",
      "\u001b[2m\u001b[36m(single_scan pid=4427)\u001b[0m 279\n",
      "\u001b[2m\u001b[36m(single_scan pid=4433)\u001b[0m 279\n",
      "\u001b[2m\u001b[36m(single_scan pid=4428)\u001b[0m 279\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 279\n",
      "scan coast:37.39975882s\n",
      "iter is  6\n",
      "gen coast:0.08770895s\n",
      "1816\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 181\n",
      "\u001b[2m\u001b[36m(single_scan pid=4424)\u001b[0m 181\n",
      "\u001b[2m\u001b[36m(single_scan pid=4430)\u001b[0m 181\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 181\n",
      "\u001b[2m\u001b[36m(single_scan pid=4425)\u001b[0m 181\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 181\n",
      "\u001b[2m\u001b[36m(single_scan pid=4432)\u001b[0m 181\n",
      "\u001b[2m\u001b[36m(single_scan pid=4427)\u001b[0m 181\n",
      "\u001b[2m\u001b[36m(single_scan pid=4433)\u001b[0m 181\n",
      "\u001b[2m\u001b[36m(single_scan pid=4428)\u001b[0m 181\n",
      "scan coast:29.41879988s\n",
      "iter is  7\n",
      "gen coast:0.04075599s\n",
      "669\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 66\n",
      "\u001b[2m\u001b[36m(single_scan pid=4424)\u001b[0m 66\n",
      "\u001b[2m\u001b[36m(single_scan pid=4430)\u001b[0m 66\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 66\n",
      "\u001b[2m\u001b[36m(single_scan pid=4425)\u001b[0m 66\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 66\n",
      "\u001b[2m\u001b[36m(single_scan pid=4432)\u001b[0m 66\n",
      "\u001b[2m\u001b[36m(single_scan pid=4427)\u001b[0m 66\n",
      "\u001b[2m\u001b[36m(single_scan pid=4433)\u001b[0m 66\n",
      "\u001b[2m\u001b[36m(single_scan pid=4428)\u001b[0m 66\n",
      "scan coast:12.20218396s\n",
      "iter is  8\n",
      "gen coast:0.00746083s\n",
      "150\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(single_scan pid=4424)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(single_scan pid=4430)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(single_scan pid=4425)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(single_scan pid=4432)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(single_scan pid=4427)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(single_scan pid=4433)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(single_scan pid=4428)\u001b[0m 15\n",
      "scan coast:3.19559121s\n",
      "iter is  9\n",
      "gen coast:0.00055003s\n",
      "30\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=4424)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=4430)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=4425)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=4432)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=4427)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=4433)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=4428)\u001b[0m 3\n",
      "scan coast:0.74758792s\n",
      "iter is  10\n",
      "gen coast:0.00002623s\n",
      "3\n",
      "scan coast:0.01228118s\n",
      "花费的时间为:121.39009905s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "L, suppData = apriori(myDat, para['minSupport'])  # 选择频繁项集\n",
    "# print(u\"频繁项集L：\", suppData)\n",
    "# print(u\"所有候选项集的支持度信息：\", suppData)\n",
    "print(f'花费的时间为:{time.time() - t:.8f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "148\n",
      "415\n",
      "662\n",
      "658\n",
      "411\n",
      "158\n",
      "37\n",
      "4\n",
      "0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4426)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4431)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=4429)\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "for i in L:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多进程实验结果\n",
    "\n",
    "不难看出，实验正确性一致。\n",
    "\n",
    "并且实验的性能得到了巨大提升。\n",
    "\n",
    "我这里使用10个进程，总效率提升了5倍。单scan部分提升了接近10倍。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
