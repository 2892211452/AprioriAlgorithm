{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# frozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def loadDataSet():\n",
    "    '''创建一个用于测试的简单的数据集'''\n",
    "\n",
    "    data = pd.read_csv(\"train.csv\")\n",
    "    data = data.sample(1000)\n",
    "#     import copy\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data)) # 将数据扩大8倍\n",
    "    print(data.shape)\n",
    "    def gerDoodAndBad(score):\n",
    "        ans = []\n",
    "\n",
    "        # math judge\n",
    "        if score['math score'] > 80:\n",
    "            ans.append('MG')\n",
    "        elif score['math score'] < 60:\n",
    "            ans.append('MB')\n",
    "\n",
    "        # reading judge\n",
    "        if score['reading score'] > 85:\n",
    "            ans.append('RG')\n",
    "        elif score['reading score'] < 50:\n",
    "            ans.append('RB')\n",
    "\n",
    "        # writing judge\n",
    "        if score['writing score'] > 85:\n",
    "            ans.append('WG')\n",
    "        elif score['writing score'] < 50:\n",
    "            ans.append('WB')\n",
    "\n",
    "        if score['test preparation course'] == 1:\n",
    "            ans.append(\"preparation\")\n",
    "\n",
    "        return ans\n",
    "\n",
    "    data['Apriori'] = data.apply(lambda x: gerDoodAndBad(x), axis=1)\n",
    "#     print(data.head())\n",
    "\n",
    "    return np.array(data['Apriori'])\n",
    "\n",
    "\n",
    "def loadTest():\n",
    "    import json\n",
    "\n",
    "    f = open(\"para.json\", 'r')\n",
    "    para = json.loads(f.read())\n",
    "    \n",
    "    testD = pd.read_csv('testData')\n",
    "    testD = testD.head(para['testDataSize'])\n",
    "    testD.shape\n",
    "    npt = np.array(testD)\n",
    "    npt\n",
    "    return npt\n",
    "\n",
    "\n",
    "# 返回只有单个元素的候选集\n",
    "def createC1(dataSet):\n",
    "    '''\n",
    "        构建初始候选项集的列表，即所有候选项集只包含一个元素，\n",
    "        C1是大小为1的所有候选项集的集合\n",
    "    '''\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if [item] not in C1:\n",
    "                C1.append([item])\n",
    "    C1.sort()\n",
    "    # return map( frozenset, C1 )\n",
    "    # return [var for var in map(frozenset,C1)]\n",
    "    return [frozenset(var) for var in C1]\n",
    "\n",
    "\n",
    "def scanD(D, Ck, minSupport):\n",
    "    '''\n",
    "        计算Ck中的项集在数据集合D(记录或者transactions)中的支持度,\n",
    "        返回满足最小支持度的项集的集合，和所有项集支持度信息的字典。\n",
    "    '''\n",
    "    print(len(Ck))\n",
    "    ssCnt = {}\n",
    "    for tid in D:  # 对于每一条transaction\n",
    "        for can in Ck:  # 对于每一个候选项集can，检查是否是transaction的一部分 # 即该候选can是否得到transaction的支持\n",
    "            flag = True\n",
    "            for i in can:\n",
    "                if i not in tid:\n",
    "                    flag = False\n",
    "                    \n",
    "            if flag:\n",
    "                ssCnt[can] = ssCnt.get(can, 0) + 1\n",
    "                \n",
    "#             if can.issubset(tid):\n",
    "#                 ssCnt[can] = ssCnt.get(can, 0) + 1\n",
    "    numItems = float(len(D))\n",
    "    # print(\"ssCnt is\",ssCnt)\n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key] / numItems  # 每个项集的支持度\n",
    "        if support >= minSupport:  # 将满足最小支持度的项集，加入retList\n",
    "            retList.insert(0, key)\n",
    "        supportData[key] = support  # 汇总支持度数据\n",
    "    return retList, supportData\n",
    "\n",
    "\n",
    "def aprioriGen(Lk, k):  # Aprior算法\n",
    "    '''\n",
    "        由初始候选项集的集合Lk生成新的生成候选项集，\n",
    "        k表示生成的新项集中所含有的元素个数\n",
    "        注意其生成的过程中，首选对每个项集按元素排序，然后每次比较两个项集，只有在k-1项相同时才将这两项合并。这样做是因为函数并非要两两合并各个集合，那样生成的集合并非都是k+1项的。在限制项数为k+1的前提下，只有在前k-1项相同、最后一项不相同的情况下合并才为所需要的新候选项集。\n",
    "    '''\n",
    "    retList = set()\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i + 1, lenLk):\n",
    "            \n",
    "            L1 = Lk[i]\n",
    "            L2 = Lk[j]\n",
    "            cnt =0\n",
    "            for m in L1:\n",
    "                if m in L2:\n",
    "                    cnt+=1\n",
    "            if cnt == k-2:\n",
    "                retList.add(Lk[i] | Lk[j])\n",
    "    return retList\n",
    "\n",
    "\n",
    "def apriori(dataSet, minSupport=0.5):\n",
    "    \"\"\"\n",
    "    该函数为Apriori算法的主函数，按照前述伪代码的逻辑执行。Ck表示项数为k的候选项集，最初的C1通过createC1()函数生成。Lk表示项数为k的频繁项集，supK为其支持度，Lk和supK由scanD()函数通过Ck计算而来。\n",
    "    :param dataSet:\n",
    "    :param minSupport:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    C1 = createC1(\n",
    "        dataSet)  # 构建初始候选项集C1  [frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]\n",
    "\n",
    "    D = [set(var) for var in dataSet]  # 集合化数据集\n",
    "    L1, suppData = scanD(D, C1, minSupport)  # 构建初始的频繁项集，即所有项集只有一个元素\n",
    "    L = [L1]  # 最初的L1中的每个项集含有一个元素，新生成的\n",
    "    # print()\n",
    "    k = 2  # 项集应该含有2个元素，所以 k=2\n",
    "\n",
    "    while (len(L[k - 2]) > 0):\n",
    "        print(\"iter is \", k)\n",
    "        t = time.time()\n",
    "        Ck = aprioriGen(L[k - 2], k)\n",
    "        print(f'gen coast:{time.time() - t:.8f}s')\n",
    "        \n",
    "        t = time.time()\n",
    "        Lk, supK = scanD(D, Ck, minSupport) # 筛选最小支持度的频繁项集\n",
    "        print(f'scan coast:{time.time() - t:.8f}s')\n",
    "        # print(\"iter is \")\n",
    "        # print(Ck)\n",
    "        # print(Lk)\n",
    "        # print()\n",
    "        suppData.update(supK)  # 将新的项集的支持度数据加入原来的总支持度字典中\n",
    "        L.append(Lk)  # 将符合最小支持度要求的项集加入L\n",
    "        k += 1  # 新生成的项集中的元素个数应不断增加\n",
    "    return L, suppData  # 返回所有满足条件的频繁项集的列表，和所有候选项集的支持度信息\n",
    "\n",
    "\n",
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):  # 规则生成与评价\n",
    "    '''\n",
    "        计算规则的可信度，返回满足最小可信度的规则。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中所有的元素\n",
    "        supportData(dic):频繁项集中所有元素的支持度\n",
    "        brl(tuple):满足可信度条件的关联规则\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    prunedH = []\n",
    "    for conseq in H:\n",
    "        conf = supportData[freqSet] / supportData[freqSet - conseq]\n",
    "        if conf >= minConf:\n",
    "            print(freqSet - conseq, '-->', conseq, 'conf:', conf)\n",
    "            brl.append((freqSet - conseq, conseq, conf))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH\n",
    "\n",
    "\n",
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    '''\n",
    "        对频繁项集中元素超过2的项集进行合并。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中的所有元素，即可以出现在规则右部的元素\n",
    "        supportData(dict):所有项集的支持度信息\n",
    "        brl(tuple):生成的规则\n",
    "    '''\n",
    "    m = len(H[0])\n",
    "    if len(freqSet) > m + 1:  # 查看频繁项集是否足够大，以到于移除大小为 m的子集，否则继续生成m+1大小的频繁项集\n",
    "        Hmp1 = aprioriGen(H, m + 1)\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)  # 对于新生成的m+1大小的频繁项集，计算新生成的关联规则的右则的集合\n",
    "        if len(Hmp1) > 1:  # 如果不止一条规则满足要求（新生成的关联规则的右则的集合的大小大于1），进一步递归合并，\n",
    "            # 这样做的结果就是会有“[1|多]->多”(右边只会是“多”，因为合并的本质是频繁子项集变大，\n",
    "            # 而calcConf函数的关联结果的右侧就是频繁子项集）的关联结果\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n",
    "\n",
    "\n",
    "def generateRules(L, supportData, minConf=0.7):\n",
    "    '''\n",
    "        根据频繁项集和最小可信度生成规则。\n",
    "        L(list):存储频繁项集\n",
    "        supportData(dict):存储着所有项集（不仅仅是频繁项集）的支持度\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    bigRuleList = []\n",
    "    for i in range(1, len(L)):\n",
    "        for freqSet in L[i]:  # 对于每一个频繁项集的集合freqSet\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            if i > 1:  # 如果频繁项集中的元素个数大于2，需要进一步合并，这样做的结果就是会有“[1|多]->多”(右边只会是“多”，\n",
    "                # 因为合并的本质是频繁子项集变大，而calcConf函数的关联结果的右侧就是频繁子项集），的关联结果\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "\n",
    "    sorted(bigRuleList)\n",
    "    return bigRuleList\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 23)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat = loadTest()  # 导入数据集\n",
    "myDat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "iter is  2\n",
      "gen coast:0.00004983s\n",
      "105\n",
      "scan coast:0.04249716s\n",
      "iter is  3\n",
      "gen coast:0.00112009s\n",
      "442\n",
      "scan coast:0.20059705s\n",
      "iter is  4\n",
      "gen coast:0.01707602s\n",
      "1225\n",
      "scan coast:0.77377486s\n",
      "iter is  5\n",
      "gen coast:0.12105179s\n",
      "2328\n",
      "scan coast:1.65006733s\n",
      "iter is  6\n",
      "gen coast:0.26737523s\n",
      "3084\n",
      "scan coast:2.27921200s\n",
      "iter is  7\n",
      "gen coast:0.56050205s\n",
      "2846\n",
      "scan coast:3.05217314s\n",
      "iter is  8\n",
      "gen coast:0.29441690s\n",
      "1797\n",
      "scan coast:1.76237488s\n",
      "iter is  9\n",
      "gen coast:0.06851888s\n",
      "745\n",
      "scan coast:0.67383814s\n",
      "iter is  10\n",
      "gen coast:0.00658298s\n",
      "187\n",
      "scan coast:0.15283298s\n",
      "iter is  11\n",
      "gen coast:0.00036097s\n",
      "24\n",
      "scan coast:0.02124429s\n",
      "iter is  12\n",
      "gen coast:0.00000978s\n",
      "1\n",
      "scan coast:0.00101399s\n",
      "花费的时间为:11.99242091s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "L, suppData = apriori(myDat, 0.3)  # 选择频繁项集\n",
    "# print(u\"频繁项集L：\", suppData)\n",
    "# print(u\"所有候选项集的支持度信息：\", suppData)\n",
    "print(f'花费的时间为:{time.time() - t:.8f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "95\n",
      "350\n",
      "811\n",
      "1261\n",
      "1341\n",
      "976\n",
      "475\n",
      "147\n",
      "26\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in L:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Apriori 多进程 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def loadDataSet():\n",
    "    '''创建一个用于测试的简单的数据集'''\n",
    "\n",
    "    data = pd.read_csv(\"train.csv\")\n",
    "    data = data.sample(1000)\n",
    "#     import copy\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data)) # 将数据扩大8倍\n",
    "    print(data.shape)\n",
    "    def gerDoodAndBad(score):\n",
    "        ans = []\n",
    "\n",
    "        # math judge\n",
    "        if score['math score'] > 80:\n",
    "            ans.append('MG')\n",
    "        elif score['math score'] < 60:\n",
    "            ans.append('MB')\n",
    "\n",
    "        # reading judge\n",
    "        if score['reading score'] > 85:\n",
    "            ans.append('RG')\n",
    "        elif score['reading score'] < 50:\n",
    "            ans.append('RB')\n",
    "\n",
    "        # writing judge\n",
    "        if score['writing score'] > 85:\n",
    "            ans.append('WG')\n",
    "        elif score['writing score'] < 50:\n",
    "            ans.append('WB')\n",
    "\n",
    "        if score['test preparation course'] == 1:\n",
    "            ans.append(\"preparation\")\n",
    "\n",
    "        return ans\n",
    "\n",
    "    data['Apriori'] = data.apply(lambda x: gerDoodAndBad(x), axis=1)\n",
    "#     print(data.head())\n",
    "\n",
    "    return np.array(data['Apriori'])\n",
    "\n",
    "\n",
    "def loadTest():\n",
    "    import json\n",
    "\n",
    "    f = open(\"para.json\", 'r')\n",
    "    para = json.loads(f.read())\n",
    "    \n",
    "    testD = pd.read_csv('testData')\n",
    "    testD = testD.head(para['testDataSize'])\n",
    "    testD.shape\n",
    "    npt = np.array(testD)\n",
    "    npt\n",
    "    return npt\n",
    "\n",
    "\n",
    "# 返回只有单个元素的候选集\n",
    "def createC1(dataSet):\n",
    "    '''\n",
    "        构建初始候选项集的列表，即所有候选项集只包含一个元素，\n",
    "        C1是大小为1的所有候选项集的集合\n",
    "    '''\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if [item] not in C1:\n",
    "                C1.append([item])\n",
    "    C1.sort()\n",
    "    # return map( frozenset, C1 )\n",
    "    # return [var for var in map(frozenset,C1)]\n",
    "    return [frozenset(var) for var in C1]\n",
    "\n",
    "\n",
    "def scanD(D, Ck, minSupport):\n",
    "    '''\n",
    "        计算Ck中的项集在数据集合D(记录或者transactions)中的支持度,\n",
    "        返回满足最小支持度的项集的集合，和所有项集支持度信息的字典。\n",
    "    '''\n",
    "    print(len(Ck))\n",
    "    ssCnt = {}\n",
    "    for tid in D:  # 对于每一条transaction\n",
    "        for can in Ck:  # 对于每一个候选项集can，检查是否是transaction的一部分 # 即该候选can是否得到transaction的支持\n",
    "            flag = True\n",
    "            for i in can:\n",
    "                if i not in tid:\n",
    "                    flag = False\n",
    "                    \n",
    "            if flag:\n",
    "                ssCnt[can] = ssCnt.get(can, 0) + 1\n",
    "                \n",
    "#             if can.issubset(tid):\n",
    "#                 ssCnt[can] = ssCnt.get(can, 0) + 1\n",
    "    numItems = float(len(D))\n",
    "    # print(\"ssCnt is\",ssCnt)\n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key] / numItems  # 每个项集的支持度\n",
    "        if support >= minSupport:  # 将满足最小支持度的项集，加入retList\n",
    "            retList.insert(0, key)\n",
    "        supportData[key] = support  # 汇总支持度数据\n",
    "    return retList, supportData\n",
    "\n",
    "import ray\n",
    "\n",
    "def total_scan(D, Ck, minSupport):\n",
    "    # 分为10核\n",
    "    Cs = []\n",
    "    Ck =list(Ck)\n",
    "    single_size = int(len(Ck)/10)\n",
    "    for i in range(10):\n",
    "        Cs.append(Ck[i* single_size : (i+1)*single_size])\n",
    "    Cs.append(Ck[10 * single_size: -1])\n",
    "    ans = [single_scan.remote(D, Ci, minSupport) for Ci in Cs]\n",
    "    ans = ray.get(ans)\n",
    "    tmp = {}\n",
    "#     print(ans)\n",
    "    for i in ans:\n",
    "        for j in i:\n",
    "            if i[j] > minSupport:\n",
    "                tmp[j] = i[j]\n",
    "#     print(tmp)\n",
    "    ret_list = [i for i in tmp]\n",
    "    return ret_list, tmp\n",
    "    \n",
    "\n",
    "@ray.remote\n",
    "def single_scan(D, Ck, minS):\n",
    "    r, c = scanD(D, Ck,minS)\n",
    "    return c\n",
    "\n",
    "\n",
    "def aprioriGen(Lk, k):  # Aprior算法\n",
    "    '''\n",
    "        由初始候选项集的集合Lk生成新的生成候选项集，\n",
    "        k表示生成的新项集中所含有的元素个数\n",
    "        注意其生成的过程中，首选对每个项集按元素排序，然后每次比较两个项集，只有在k-1项相同时才将这两项合并。这样做是因为函数并非要两两合并各个集合，那样生成的集合并非都是k+1项的。在限制项数为k+1的前提下，只有在前k-1项相同、最后一项不相同的情况下合并才为所需要的新候选项集。\n",
    "    '''\n",
    "    retList = set()\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i + 1, lenLk):\n",
    "            \n",
    "            L1 = Lk[i]\n",
    "            L2 = Lk[j]\n",
    "            cnt =0\n",
    "            for m in L1:\n",
    "                if m in L2:\n",
    "                    cnt+=1\n",
    "            if cnt == k-2:\n",
    "                retList.add(Lk[i] | Lk[j])\n",
    "    return retList\n",
    "\n",
    "\n",
    "def apriori(dataSet, minSupport=0.5):\n",
    "    \"\"\"\n",
    "    该函数为Apriori算法的主函数，按照前述伪代码的逻辑执行。Ck表示项数为k的候选项集，最初的C1通过createC1()函数生成。Lk表示项数为k的频繁项集，supK为其支持度，Lk和supK由scanD()函数通过Ck计算而来。\n",
    "    :param dataSet:\n",
    "    :param minSupport:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    C1 = createC1(\n",
    "        dataSet)  # 构建初始候选项集C1  [frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]\n",
    "\n",
    "    D = [set(var) for var in dataSet]  # 集合化数据集\n",
    "    L1, suppData = scanD(D, C1, minSupport)  # 构建初始的频繁项集，即所有项集只有一个元素\n",
    "    L = [L1]  # 最初的L1中的每个项集含有一个元素，新生成的\n",
    "    # print()\n",
    "    k = 2  # 项集应该含有2个元素，所以 k=2\n",
    "\n",
    "    while (len(L[k - 2]) > 0):\n",
    "        print(\"iter is \", k)\n",
    "        t = time.time()\n",
    "        Ck = aprioriGen(L[k - 2], k)\n",
    "        print(f'gen coast:{time.time() - t:.8f}s')\n",
    "        \n",
    "        t = time.time()\n",
    "        Lk, supK = total_scan(D, Ck, minSupport) # 筛选最小支持度的频繁项集\n",
    "        print(f'scan coast:{time.time() - t:.8f}s')\n",
    "        # print(\"iter is \")\n",
    "        # print(Ck)\n",
    "        # print(Lk)\n",
    "        # print()\n",
    "        suppData.update(supK)  # 将新的项集的支持度数据加入原来的总支持度字典中\n",
    "        L.append(Lk)  # 将符合最小支持度要求的项集加入L\n",
    "        k += 1  # 新生成的项集中的元素个数应不断增加\n",
    "    return L, suppData  # 返回所有满足条件的频繁项集的列表，和所有候选项集的支持度信息\n",
    "\n",
    "\n",
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):  # 规则生成与评价\n",
    "    '''\n",
    "        计算规则的可信度，返回满足最小可信度的规则。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中所有的元素\n",
    "        supportData(dic):频繁项集中所有元素的支持度\n",
    "        brl(tuple):满足可信度条件的关联规则\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    prunedH = []\n",
    "    for conseq in H:\n",
    "        conf = supportData[freqSet] / supportData[freqSet - conseq]\n",
    "        if conf >= minConf:\n",
    "            print(freqSet - conseq, '-->', conseq, 'conf:', conf)\n",
    "            brl.append((freqSet - conseq, conseq, conf))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH\n",
    "\n",
    "\n",
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    '''\n",
    "        对频繁项集中元素超过2的项集进行合并。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中的所有元素，即可以出现在规则右部的元素\n",
    "        supportData(dict):所有项集的支持度信息\n",
    "        brl(tuple):生成的规则\n",
    "    '''\n",
    "    m = len(H[0])\n",
    "    if len(freqSet) > m + 1:  # 查看频繁项集是否足够大，以到于移除大小为 m的子集，否则继续生成m+1大小的频繁项集\n",
    "        Hmp1 = aprioriGen(H, m + 1)\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)  # 对于新生成的m+1大小的频繁项集，计算新生成的关联规则的右则的集合\n",
    "        if len(Hmp1) > 1:  # 如果不止一条规则满足要求（新生成的关联规则的右则的集合的大小大于1），进一步递归合并，\n",
    "            # 这样做的结果就是会有“[1|多]->多”(右边只会是“多”，因为合并的本质是频繁子项集变大，\n",
    "            # 而calcConf函数的关联结果的右侧就是频繁子项集）的关联结果\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n",
    "\n",
    "\n",
    "def generateRules(L, supportData, minConf=0.7):\n",
    "    '''\n",
    "        根据频繁项集和最小可信度生成规则。\n",
    "        L(list):存储频繁项集\n",
    "        supportData(dict):存储着所有项集（不仅仅是频繁项集）的支持度\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    bigRuleList = []\n",
    "    for i in range(1, len(L)):\n",
    "        for freqSet in L[i]:  # 对于每一个频繁项集的集合freqSet\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            if i > 1:  # 如果频繁项集中的元素个数大于2，需要进一步合并，这样做的结果就是会有“[1|多]->多”(右边只会是“多”，\n",
    "                # 因为合并的本质是频繁子项集变大，而calcConf函数的关联结果的右侧就是频繁子项集），的关联结果\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "\n",
    "    sorted(bigRuleList)\n",
    "    return bigRuleList\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 23)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat = loadTest()  # 导入数据集\n",
    "myDat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "iter is  2\n",
      "gen coast:0.00006008s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 10\n",
      "scan coast:0.06613421s\n",
      "iter is  3\n",
      "gen coast:0.00113177s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37283)\u001b[0m 44\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 44\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 44\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 44\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 44\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 44\n",
      "\u001b[2m\u001b[36m(single_scan pid=37281)\u001b[0m 44\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 44\n",
      "scan coast:0.08554077s\n",
      "iter is  4\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 44\n",
      "\u001b[2m\u001b[36m(single_scan pid=37284)\u001b[0m 44\n",
      "gen coast:0.01929092s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37285)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37283)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(single_scan pid=37285)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(single_scan pid=37287)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(single_scan pid=37281)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(single_scan pid=37284)\u001b[0m 122\n",
      "\u001b[2m\u001b[36m(single_scan pid=37282)\u001b[0m 122\n",
      "scan coast:0.18337417s\n",
      "iter is  5\n",
      "gen coast:0.10745096s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37282)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37283)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37285)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37287)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37281)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37284)\u001b[0m 232\n",
      "\u001b[2m\u001b[36m(single_scan pid=37287)\u001b[0m 3\n",
      "scan coast:0.37393093s\n",
      "iter is  6\n",
      "gen coast:0.35729885s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37281)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37282)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37283)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37285)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37287)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37284)\u001b[0m 308\n",
      "\u001b[2m\u001b[36m(single_scan pid=37281)\u001b[0m 3\n",
      "scan coast:0.51655483s\n",
      "iter is  7\n",
      "gen coast:0.41349196s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37282)\u001b[0m 284\n",
      "\u001b[2m\u001b[36m(single_scan pid=37283)\u001b[0m 284\n",
      "\u001b[2m\u001b[36m(single_scan pid=37285)\u001b[0m 284\n",
      "\u001b[2m\u001b[36m(single_scan pid=37287)\u001b[0m 284\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 284\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 284\n",
      "\u001b[2m\u001b[36m(single_scan pid=37281)\u001b[0m 284\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 284\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 284\n",
      "\u001b[2m\u001b[36m(single_scan pid=37284)\u001b[0m 284\n",
      "scan coast:0.35253811s\n",
      "iter is  8\n",
      "\u001b[2m\u001b[36m(single_scan pid=37282)\u001b[0m 1\n",
      "gen coast:0.24109173s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37282)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37283)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37285)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37287)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37281)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37284)\u001b[0m 179\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 6\n",
      "scan coast:0.37797523s\n",
      "iter is  9\n",
      "gen coast:0.06639266s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37282)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37283)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37285)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37287)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37281)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37284)\u001b[0m 74\n",
      "\u001b[2m\u001b[36m(single_scan pid=37282)\u001b[0m 4\n",
      "scan coast:0.18492699s\n",
      "iter is  10\n",
      "gen coast:0.00743222s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37283)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(single_scan pid=37285)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 18\n",
      "scan coast:0.07973099s\u001b[2m\u001b[36m(single_scan pid=37280)\u001b[0m 18\n",
      "\n",
      "iter is  11\n",
      "gen coast:0.00036907s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 6\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 18\n",
      "scan coast:0.05867505s\n",
      "iter is  12\n",
      "gen coast:0.00002885s\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37278)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37279)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 0scan coast:0.06337976s\n",
      "花费的时间为:3.60519791s\n",
      "\n",
      "\u001b[2m\u001b[36m(single_scan pid=37286)\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "L, suppData = apriori(myDat, 0.3)  # 选择频繁项集\n",
    "# print(u\"频繁项集L：\", suppData)\n",
    "# print(u\"所有候选项集的支持度信息：\", suppData)\n",
    "print(f'花费的时间为:{time.time() - t:.8f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "95\n",
      "350\n",
      "811\n",
      "1261\n",
      "1341\n",
      "976\n",
      "475\n",
      "147\n",
      "26\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in L:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多进程实验结果\n",
    "\n",
    "不难看出，实验正确性一致。\n",
    "\n",
    "并且实验的性能得到了巨大提升。\n",
    "\n",
    "我这里使用10个进程，总效率提升了5倍。单scan部分提升了接近10倍。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
