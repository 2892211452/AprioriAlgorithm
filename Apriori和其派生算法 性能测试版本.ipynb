{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# frozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def loadDataSet():\n",
    "    '''创建一个用于测试的简单的数据集'''\n",
    "\n",
    "    data = pd.read_csv(\"train.csv\")\n",
    "    data = data.sample(1000)\n",
    "#     import copy\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data)) # 将数据扩大8倍\n",
    "    print(data.shape)\n",
    "    def gerDoodAndBad(score):\n",
    "        ans = []\n",
    "\n",
    "        # math judge\n",
    "        if score['math score'] > 80:\n",
    "            ans.append('MG')\n",
    "        elif score['math score'] < 60:\n",
    "            ans.append('MB')\n",
    "\n",
    "        # reading judge\n",
    "        if score['reading score'] > 85:\n",
    "            ans.append('RG')\n",
    "        elif score['reading score'] < 50:\n",
    "            ans.append('RB')\n",
    "\n",
    "        # writing judge\n",
    "        if score['writing score'] > 85:\n",
    "            ans.append('WG')\n",
    "        elif score['writing score'] < 50:\n",
    "            ans.append('WB')\n",
    "\n",
    "        if score['test preparation course'] == 1:\n",
    "            ans.append(\"preparation\")\n",
    "\n",
    "        return ans\n",
    "\n",
    "    data['Apriori'] = data.apply(lambda x: gerDoodAndBad(x), axis=1)\n",
    "#     print(data.head())\n",
    "\n",
    "    return np.array(data['Apriori'])\n",
    "\n",
    "\n",
    "def loadTest():\n",
    "    import json\n",
    "\n",
    "    f = open(\"para.json\", 'r')\n",
    "    para = json.loads(f.read())\n",
    "    \n",
    "    testD = pd.read_csv('testData')\n",
    "    testD = testD.head(para['testDataSize'])\n",
    "    testD.shape\n",
    "    npt = np.array(testD)\n",
    "    npt\n",
    "    return npt\n",
    "\n",
    "\n",
    "# 返回只有单个元素的候选集\n",
    "def createC1(dataSet):\n",
    "    '''\n",
    "        构建初始候选项集的列表，即所有候选项集只包含一个元素，\n",
    "        C1是大小为1的所有候选项集的集合\n",
    "    '''\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if [item] not in C1:\n",
    "                C1.append([item])\n",
    "    C1.sort()\n",
    "    # return map( frozenset, C1 )\n",
    "    # return [var for var in map(frozenset,C1)]\n",
    "    return [frozenset(var) for var in C1]\n",
    "\n",
    "\n",
    "def scanD(D, Ck, minSupport):\n",
    "    '''\n",
    "        计算Ck中的项集在数据集合D(记录或者transactions)中的支持度,\n",
    "        返回满足最小支持度的项集的集合，和所有项集支持度信息的字典。\n",
    "    '''\n",
    "    print(len(Ck))\n",
    "    ssCnt = {}\n",
    "    for tid in D:  # 对于每一条transaction\n",
    "        for can in Ck:  # 对于每一个候选项集can，检查是否是transaction的一部分 # 即该候选can是否得到transaction的支持\n",
    "            flag = True\n",
    "            for i in can:\n",
    "                if i not in tid:\n",
    "                    flag = False\n",
    "                    \n",
    "            if flag:\n",
    "                ssCnt[can] = ssCnt.get(can, 0) + 1\n",
    "                \n",
    "#             if can.issubset(tid):\n",
    "#                 ssCnt[can] = ssCnt.get(can, 0) + 1\n",
    "    numItems = float(len(D))\n",
    "    # print(\"ssCnt is\",ssCnt)\n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key] / numItems  # 每个项集的支持度\n",
    "        if support >= minSupport:  # 将满足最小支持度的项集，加入retList\n",
    "            retList.insert(0, key)\n",
    "        supportData[key] = support  # 汇总支持度数据\n",
    "    return retList, supportData\n",
    "\n",
    "\n",
    "def aprioriGen(Lk, k):  # Aprior算法\n",
    "    '''\n",
    "        由初始候选项集的集合Lk生成新的生成候选项集，\n",
    "        k表示生成的新项集中所含有的元素个数\n",
    "        注意其生成的过程中，首选对每个项集按元素排序，然后每次比较两个项集，只有在k-1项相同时才将这两项合并。这样做是因为函数并非要两两合并各个集合，那样生成的集合并非都是k+1项的。在限制项数为k+1的前提下，只有在前k-1项相同、最后一项不相同的情况下合并才为所需要的新候选项集。\n",
    "    '''\n",
    "    retList = set()\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i + 1, lenLk):\n",
    "            \n",
    "            L1 = Lk[i]\n",
    "            L2 = Lk[j]\n",
    "            cnt =0\n",
    "            for m in L1:\n",
    "                if m in L2:\n",
    "                    cnt+=1\n",
    "            if cnt == k-2:\n",
    "                retList.add(Lk[i] | Lk[j])\n",
    "    return retList\n",
    "\n",
    "\n",
    "def apriori(dataSet, minSupport=0.5):\n",
    "    \"\"\"\n",
    "    该函数为Apriori算法的主函数，按照前述伪代码的逻辑执行。Ck表示项数为k的候选项集，最初的C1通过createC1()函数生成。Lk表示项数为k的频繁项集，supK为其支持度，Lk和supK由scanD()函数通过Ck计算而来。\n",
    "    :param dataSet:\n",
    "    :param minSupport:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    C1 = createC1(\n",
    "        dataSet)  # 构建初始候选项集C1  [frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]\n",
    "\n",
    "    D = [set(var) for var in dataSet]  # 集合化数据集\n",
    "    L1, suppData = scanD(D, C1, minSupport)  # 构建初始的频繁项集，即所有项集只有一个元素\n",
    "    L = [L1]  # 最初的L1中的每个项集含有一个元素，新生成的\n",
    "    # print()\n",
    "    k = 2  # 项集应该含有2个元素，所以 k=2\n",
    "\n",
    "    while (len(L[k - 2]) > 0):\n",
    "        print(\"iter is \", k)\n",
    "        t = time.time()\n",
    "        Ck = aprioriGen(L[k - 2], k)\n",
    "        print(f'gen coast:{time.time() - t:.8f}s')\n",
    "        \n",
    "        t = time.time()\n",
    "        Lk, supK = scanD(D, Ck, minSupport) # 筛选最小支持度的频繁项集\n",
    "        print(f'scan coast:{time.time() - t:.8f}s')\n",
    "        # print(\"iter is \")\n",
    "        # print(Ck)\n",
    "        # print(Lk)\n",
    "        # print()\n",
    "        suppData.update(supK)  # 将新的项集的支持度数据加入原来的总支持度字典中\n",
    "        L.append(Lk)  # 将符合最小支持度要求的项集加入L\n",
    "        k += 1  # 新生成的项集中的元素个数应不断增加\n",
    "    return L, suppData  # 返回所有满足条件的频繁项集的列表，和所有候选项集的支持度信息\n",
    "\n",
    "\n",
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):  # 规则生成与评价\n",
    "    '''\n",
    "        计算规则的可信度，返回满足最小可信度的规则。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中所有的元素\n",
    "        supportData(dic):频繁项集中所有元素的支持度\n",
    "        brl(tuple):满足可信度条件的关联规则\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    prunedH = []\n",
    "    for conseq in H:\n",
    "        conf = supportData[freqSet] / supportData[freqSet - conseq]\n",
    "        if conf >= minConf:\n",
    "            print(freqSet - conseq, '-->', conseq, 'conf:', conf)\n",
    "            brl.append((freqSet - conseq, conseq, conf))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH\n",
    "\n",
    "\n",
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    '''\n",
    "        对频繁项集中元素超过2的项集进行合并。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中的所有元素，即可以出现在规则右部的元素\n",
    "        supportData(dict):所有项集的支持度信息\n",
    "        brl(tuple):生成的规则\n",
    "    '''\n",
    "    m = len(H[0])\n",
    "    if len(freqSet) > m + 1:  # 查看频繁项集是否足够大，以到于移除大小为 m的子集，否则继续生成m+1大小的频繁项集\n",
    "        Hmp1 = aprioriGen(H, m + 1)\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)  # 对于新生成的m+1大小的频繁项集，计算新生成的关联规则的右则的集合\n",
    "        if len(Hmp1) > 1:  # 如果不止一条规则满足要求（新生成的关联规则的右则的集合的大小大于1），进一步递归合并，\n",
    "            # 这样做的结果就是会有“[1|多]->多”(右边只会是“多”，因为合并的本质是频繁子项集变大，\n",
    "            # 而calcConf函数的关联结果的右侧就是频繁子项集）的关联结果\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n",
    "\n",
    "\n",
    "def generateRules(L, supportData, minConf=0.7):\n",
    "    '''\n",
    "        根据频繁项集和最小可信度生成规则。\n",
    "        L(list):存储频繁项集\n",
    "        supportData(dict):存储着所有项集（不仅仅是频繁项集）的支持度\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    bigRuleList = []\n",
    "    for i in range(1, len(L)):\n",
    "        for freqSet in L[i]:  # 对于每一个频繁项集的集合freqSet\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            if i > 1:  # 如果频繁项集中的元素个数大于2，需要进一步合并，这样做的结果就是会有“[1|多]->多”(右边只会是“多”，\n",
    "                # 因为合并的本质是频繁子项集变大，而calcConf函数的关联结果的右侧就是频繁子项集），的关联结果\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "\n",
    "    sorted(bigRuleList)\n",
    "    return bigRuleList\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat = loadTest()  # 导入数据集\n",
    "myDat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "iter is  2\n",
      "gen coast:0.00000000s\n",
      "120\n",
      "scan coast:0.12128401s\n",
      "iter is  3\n",
      "gen coast:0.00804901s\n",
      "529\n",
      "scan coast:0.55353427s\n",
      "iter is  4\n",
      "gen coast:0.15910244s\n",
      "1506\n",
      "scan coast:1.58464718s\n",
      "iter is  5\n",
      "gen coast:0.48731828s\n",
      "2911\n",
      "scan coast:3.23005486s\n",
      "iter is  6\n",
      "gen coast:1.41848564s\n",
      "3914\n",
      "scan coast:4.83819628s\n",
      "iter is  7\n",
      "gen coast:1.75707054s\n",
      "3675\n",
      "scan coast:4.81357265s\n",
      "iter is  8\n",
      "gen coast:1.03155327s\n",
      "2369\n",
      "scan coast:3.23273921s\n",
      "iter is  9\n",
      "gen coast:0.32773948s\n",
      "1001\n",
      "scan coast:1.62619948s\n",
      "iter is  10\n",
      "gen coast:0.02393150s\n",
      "250\n",
      "scan coast:0.39178705s\n",
      "iter is  11\n",
      "gen coast:0.00000000s\n",
      "28\n",
      "scan coast:0.04792452s\n",
      "iter is  12\n",
      "gen coast:0.00000000s\n",
      "0\n",
      "scan coast:0.00000000s\n",
      "花费的时间为:25.72822475s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "L, suppData = apriori(myDat, 0.3)  # 选择频繁项集\n",
    "# print(u\"频繁项集L：\", suppData)\n",
    "# print(u\"所有候选项集的支持度信息：\", suppData)\n",
    "print(f'花费的时间为:{time.time() - t:.8f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = generateRules(L, suppData, minConf=0.99)\n",
    "# print('rules:\\n', rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBE Apriori二进制编码算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class node:\n",
    "    def __int__(self):\n",
    "        self.items = []\n",
    "        self.key = 0\n",
    "        self.sup = 0\n",
    "    def show(self):\n",
    "        print(self.items, bin(self.key), end=\" \")\n",
    "        try:\n",
    "            print(self.sup)\n",
    "        except:\n",
    "            print()\n",
    "            pass\n",
    "\n",
    "\n",
    "def ListToNode(items):\n",
    "    nodeT = node()\n",
    "    nodeT.key =0\n",
    "    nodeT.items = items\n",
    "    for index,i in enumerate(defaultSL): # 进行编码\n",
    "        if i in items:\n",
    "            nodeT.key += 1 << (len(defaultSL) - index -1)\n",
    "    return nodeT\n",
    "\n",
    "\n",
    "def keyToList(key):\n",
    "    ans =[]\n",
    "    global defaultSL\n",
    "    for i in range(1,len(defaultSL) +1):\n",
    "        if key  & (1 << (len(defaultSL)) - i) == (1 << (len(defaultSL)) - i): # 含有第i个\n",
    "            ans.append(defaultSL[i-1])\n",
    "    return ans\n",
    "\n",
    "\n",
    "def loadDataSet():\n",
    "    '''创建一个用于测试的简单的数据集'''\n",
    "\n",
    "    data = pd.read_csv(\"train.csv\")\n",
    "    data = data.sample(1000)\n",
    "#     import copy\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data))\n",
    "#     data = data.append(copy.deepcopy(data)) # 将数据扩大8倍\n",
    "    print(data.shape)\n",
    "    def gerDoodAndBad(score):\n",
    "        ans = []\n",
    "\n",
    "        # math judge\n",
    "        if score['math score'] > 80:\n",
    "            ans.append('MG')\n",
    "        elif score['math score'] < 60:\n",
    "            ans.append('MB')\n",
    "\n",
    "        # reading judge\n",
    "        if score['reading score'] > 85:\n",
    "            ans.append('RG')\n",
    "        elif score['reading score'] < 50:\n",
    "            ans.append('RB')\n",
    "\n",
    "        # writing judge\n",
    "        if score['writing score'] > 85:\n",
    "            ans.append('WG')\n",
    "        elif score['writing score'] < 50:\n",
    "            ans.append('WB')\n",
    "\n",
    "        if score['test preparation course'] == 1:\n",
    "            ans.append(\"preparation\")\n",
    "\n",
    "        return ans\n",
    "\n",
    "    data['Apriori'] = data.apply(lambda x: gerDoodAndBad(x), axis=1)\n",
    "#     print(data.head())\n",
    "\n",
    "    return np.array(data['Apriori'])\n",
    "\n",
    "def loadTest():\n",
    "    import json\n",
    "\n",
    "    f = open(\"para.json\", 'r')\n",
    "    para = json.loads(f.read())\n",
    "    \n",
    "    testD = pd.read_csv('testData')\n",
    "    testD = testD.head(para['testDataSize'])\n",
    "    testD.shape\n",
    "    npt = np.array(testD)\n",
    "    npt\n",
    "    return npt\n",
    "\n",
    "# 返回只有单个元素的候选集\n",
    "def createC1(dataSet):\n",
    "    '''\n",
    "        构建初始候选项集的列表，即所有候选项集只包含一个元素，\n",
    "        C1是大小为1的所有候选项集的集合\n",
    "    '''\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if [item] not in C1:\n",
    "                C1.append([item])\n",
    "    C1.sort()\n",
    "    # return map( frozenset, C1 )\n",
    "    # return [var for var in map(frozenset,C1)]\n",
    "    return [frozenset(var) for var in C1]\n",
    "\n",
    "\n",
    "def scanD(D, Ck, minSupport):\n",
    "    '''\n",
    "        计算Ck中的项集在数据集合D(记录或者transactions)中的支持度,\n",
    "        返回满足最小支持度的项集的集合，和所有项集支持度信息的字典。\n",
    "    '''\n",
    "    print(len(Ck))\n",
    "\n",
    "    for i in Ck:\n",
    "        i.sup =0\n",
    "        for item in D:\n",
    "            if i.key & item.key == i.key: #与运算判断子集\n",
    "                i.sup +=1\n",
    "    ans =[]\n",
    "    length = len(D)\n",
    "    for i in Ck:\n",
    "        i.sup = i.sup / length\n",
    "        if i.sup >= minSupport:\n",
    "            ans.append(i)\n",
    "            # i.show()\n",
    "    return ans\n",
    "\n",
    "\n",
    "\n",
    "def aprioriGen(Lk, k):  # Aprior算法\n",
    "    '''\n",
    "        由初始候选项集的集合Lk生成新的生成候选项集，\n",
    "        k表示生成的新项集中所含有的元素个数\n",
    "        注意其生成的过程中，首选对每个项集按元素排序，然后每次比较两个项集，只有在前k-1项相同时才将这两项合并。这样做是因为函数并非要两两合并各个集合，那样生成的集合并非都是k+1项的。在限制项数为k+1的前提下，只有在前k-1项相同、最后一项不相同的情况下合并才为所需要的新候选项集。\n",
    "    '''\n",
    "\n",
    "    global twoDif\n",
    "\n",
    "    retList = []\n",
    "    lenLk = len(Lk)\n",
    "    nowSet = set()\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i + 1, lenLk):\n",
    "\n",
    "            a = Lk[i]\n",
    "            b = Lk[j]\n",
    "            t = a.key ^ b.key\n",
    "            #要注意候选集以及去重的问题\n",
    "            if t in twoDif:\n",
    "\n",
    "                \n",
    "                tkey = a.key | b.key\n",
    "                if tkey not in nowSet:\n",
    "                    nodeT = node()\n",
    "                    nodeT.key = tkey\n",
    "                    nowSet.add(nodeT.key)\n",
    "                    # a.show()\n",
    "                    # b.show()\n",
    "#                     nodeT.items = keyToList(nodeT.key)  #生成新的items\n",
    "                    retList.append(nodeT)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    return retList\n",
    "\n",
    "\n",
    "def apriori(dataSet, minSupport=0.5):\n",
    "    \"\"\"\n",
    "    该函数为Apriori算法的主函数，按照前述伪代码的逻辑执行。Ck表示项数为k的候选项集，最初的C1通过createC1()函数生成。Lk表示项数为k的频繁项集，supK为其支持度，Lk和supK由scanD()函数通过Ck计算而来。\n",
    "    :param dataSet:\n",
    "    :param minSupport:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # C1 = createC1(dataSet)  # 构建初始候选项集C1  [frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]\n",
    "    global defaultSL\n",
    "    global defaultS\n",
    "\n",
    "#     print(defaultSL)\n",
    "    C1 = [ListToNode([i]) for i in defaultSL]\n",
    "\n",
    "\n",
    "    # D = [set(var) for var in dataSet]  # 集合化数据集\n",
    "\n",
    "    F1 = scanD(dataSet, C1, minSupport)  # 构建初始的频繁项集，即所有项集只有一个元素\n",
    "\n",
    "    # print()\n",
    "    L = [F1]\n",
    "    k = 2  # 项集应该含有2个元素，所以 k=2\n",
    "\n",
    "    while (len(L[k - 2]) > 0):\n",
    "        print(\"iter is \", k)\n",
    "        t= time.time()\n",
    "        Ck = aprioriGen(L[k - 2], k) # 计算候选集\n",
    "        print(f'gen coast:{time.time() - t:.8f}s')\n",
    "        t = time.time()\n",
    "        \n",
    "        Fk= scanD(dataSet, Ck, minSupport) # 筛选最小支持度的频繁项集\n",
    "        print(f'scan coast:{time.time() - t:.8f}s')\n",
    "\n",
    "        L.append(Fk)  # 将符合最小支持度要求的项集加入L\n",
    "        k += 1  # 新生成的项集中的元素个数应不断增加\n",
    "    return L  # 返回所有满足条件的频繁项集的列表，和所有候选项集的支持度信息\n",
    "\n",
    "\n",
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):  # 规则生成与评价\n",
    "    '''\n",
    "        计算规则的可信度，返回满足最小可信度的规则。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中所有的元素\n",
    "        supportData(dic):频繁项集中所有元素的支持度\n",
    "        brl(tuple):满足可信度条件的关联规则\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    prunedH = []\n",
    "    for conseq in H:\n",
    "        conf = supportData[freqSet] / supportData[freqSet - conseq]\n",
    "        if conf >= minConf:\n",
    "            print(freqSet - conseq, '-->', conseq, 'conf:', conf)\n",
    "            brl.append((freqSet - conseq, conseq, conf))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH\n",
    "\n",
    "\n",
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    '''\n",
    "        对频繁项集中元素超过2的项集进行合并。\n",
    "        freqSet(frozenset):频繁项集\n",
    "        H(frozenset):频繁项集中的所有元素，即可以出现在规则右部的元素\n",
    "        supportData(dict):所有项集的支持度信息\n",
    "        brl(tuple):生成的规则\n",
    "    '''\n",
    "    m = len(H[0])\n",
    "    if len(freqSet) > m + 1:  # 查看频繁项集是否足够大，以到于移除大小为 m的子集，否则继续生成m+1大小的频繁项集\n",
    "        Hmp1 = aprioriGen(H, m + 1)\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)  # 对于新生成的m+1大小的频繁项集，计算新生成的关联规则的右则的集合\n",
    "        if len(Hmp1) > 1:  # 如果不止一条规则满足要求（新生成的关联规则的右则的集合的大小大于1），进一步递归合并，\n",
    "            # 这样做的结果就是会有“[1|多]->多”(右边只会是“多”，因为合并的本质是频繁子项集变大，\n",
    "            # 而calcConf函数的关联结果的右侧就是频繁子项集）的关联结果\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n",
    "\n",
    "\n",
    "def generateRules(L, supportData, minConf=0.7):\n",
    "    '''\n",
    "        根据频繁项集和最小可信度生成规则。\n",
    "        L(list):存储频繁项集\n",
    "        supportData(dict):存储着所有项集（不仅仅是频繁项集）的支持度\n",
    "        minConf(float):最小可信度\n",
    "    '''\n",
    "    bigRuleList = []\n",
    "    for i in range(1, len(L)):\n",
    "        for freqSet in L[i]:  # 对于每一个频繁项集的集合freqSet\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            if i > 1:  # 如果频繁项集中的元素个数大于2，需要进一步合并，这样做的结果就是会有“[1|多]->多”(右边只会是“多”，\n",
    "                # 因为合并的本质是频繁子项集变大，而calcConf函数的关联结果的右侧就是频繁子项集），的关联结果\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "\n",
    "    sorted(bigRuleList)\n",
    "    return bigRuleList\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] 22\n"
     ]
    }
   ],
   "source": [
    "defaultS = set()  # 存储所有单项的集合\n",
    "myDat = loadTest()  # 导入数据集\n",
    "for i in myDat:\n",
    "    for j in i:\n",
    "        if j not in defaultS:\n",
    "            defaultS.add(j)\n",
    "defaultSL = list(defaultS)\n",
    "defaultSL = sorted(defaultSL) # 把所有单项计算出来并排序，形成默认顺序，方便后面进行二进制编码\n",
    "print(defaultSL, len(defaultSL))\n",
    "Items = []\n",
    "\n",
    "\n",
    "\n",
    "#对项集进行二进制编码\n",
    "for items in myDat:\n",
    "    nodeT = node()\n",
    "    nodeT.key =0\n",
    "    nodeT.items = items\n",
    "    for index,i in enumerate(defaultSL): # 进行编码\n",
    "        if i in items:\n",
    "            nodeT.key += 1 << (len(defaultSL) - index -1)\n",
    "    Items.append(nodeT)\n",
    "    # print(items, bin(nodeT.key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到默认编码序列['MB', 'MG', 'RB', 'RG', 'WB', 'WG', 'preparation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b11000000000\n",
      "0b1000000001\n",
      "0b10000000001\n",
      "0b11\n",
      "0b100000000001\n",
      "0b101\n",
      "0b1000000000001\n",
      "0b10000000000001\n",
      "0b100000000000001\n",
      "0b1001\n",
      "0b1000000000000001\n",
      "0b10000000000000001\n",
      "0b10000000000000000001\n",
      "0b1000000000000000001\n",
      "0b1010\n",
      "0b1100\n",
      "0b1000000001000\n",
      "0b10001\n",
      "0b10010\n",
      "0b100000000000000010\n",
      "0b10100\n",
      "0b100000000000100\n",
      "0b1000000000000100\n",
      "0b10000000000000100\n",
      "0b100000000000000100\n",
      "0b1000000000000000100\n",
      "0b10000000000000000100\n",
      "0b1000000010\n",
      "0b100000000000000000100\n",
      "0b1000000000000000000100\n",
      "0b11000\n",
      "0b100000000000000000001\n",
      "0b1000000000000000000001\n",
      "0b100001\n",
      "0b110\n",
      "0b100010\n",
      "0b100100\n",
      "0b1000000100\n",
      "0b1000100000\n",
      "0b10000100000\n",
      "0b101000\n",
      "0b100000000001000\n",
      "0b1000000000001000\n",
      "0b10000000010\n",
      "0b10000000000001000\n",
      "0b100000000000001000\n",
      "0b1000000000000001000\n",
      "0b10000000000000001000\n",
      "0b100000000000000001000\n",
      "0b1000000000000000001000\n",
      "0b110000\n",
      "0b10000000100\n",
      "0b1100000000000000\n",
      "0b10000000000000000010\n",
      "0b100000000000000000010\n",
      "0b1000000000000000000010\n",
      "0b1000001000\n",
      "0b1001000000\n",
      "0b1000001\n",
      "0b1000010\n",
      "0b10001000000\n",
      "0b1000100\n",
      "0b100001000000\n",
      "0b1000001000000\n",
      "0b10000001000000\n",
      "0b100000000000010\n",
      "0b1001000\n",
      "0b10000001000\n",
      "0b100000000010\n",
      "0b1000000100000000000\n",
      "0b10010000000000000000\n",
      "0b100010000000000000000\n",
      "0b1000010000000000000\n",
      "0b1010000\n",
      "0b100000000010000\n",
      "0b1000000000010000\n",
      "0b10000000000010000\n",
      "0b100000000000010000\n",
      "0b100000000100\n",
      "0b1000000000000010000\n",
      "0b10000000000000010000\n",
      "0b1000000000000010\n",
      "0b100000000000000010000\n",
      "0b1000000000000000010000\n",
      "0b10000000001000000000\n",
      "0b100000000001000000000\n",
      "0b1000000000001000000000\n",
      "0b10100000000000000\n",
      "0b1100000\n",
      "0b1000010000\n",
      "0b110000000000\n",
      "0b100001000000000\n",
      "0b11000000000000000\n",
      "0b100000001000\n",
      "0b1010000000000000000000\n",
      "0b10000010000\n",
      "0b1000001000000000\n",
      "0b10000000000000010\n",
      "0b1100000000000000000000\n",
      "0b10000100000000000000\n",
      "0b1010000000\n",
      "0b10000001\n",
      "0b10000010\n",
      "0b10010000000\n",
      "0b10000100\n",
      "0b100010000000\n",
      "0b1000010000000\n",
      "0b10000010000000\n",
      "0b10001000\n",
      "0b100000010000000\n",
      "0b1000000010000000\n",
      "0b1000000000010\n",
      "0b10000000010000000\n",
      "0b100000000010000000\n",
      "0b1000000000010000000\n",
      "0b10000001000000000000\n",
      "0b10010000\n",
      "0b100000010000\n",
      "0b1001000000000\n",
      "0b100000001000000000000\n",
      "0b100000100000000000000\n",
      "0b1000000000100\n",
      "0b10000001000000000\n",
      "0b100100000000000000\n",
      "0b10000000000010000000\n",
      "0b100000000000010000000\n",
      "0b1000000000000010000000\n",
      "0b101000000000000\n",
      "0b100000000000000001\n",
      "0b10100000\n",
      "0b1010000000000\n",
      "0b100000000100000\n",
      "0b1000000000100000\n",
      "0b10000000000100000\n",
      "0b100000000000100000\n",
      "0b1000000000000100000\n",
      "0b10000000000000100000\n",
      "0b100000000000000100000\n",
      "0b1000000000000000100000\n",
      "0b10000000010000000000\n",
      "0b100000000010000000000\n",
      "0b1000000000010000000000\n",
      "0b1001000000000000\n",
      "0b101000000000000000\n",
      "0b1000000100000000000000\n",
      "0b10100000000000000000\n",
      "0b100010000000000\n",
      "0b11000000\n",
      "0b1100000000000\n",
      "0b100100000000000000000\n",
      "0b110000000000000000\n",
      "0b1000010000000000\n",
      "0b10001000000000000\n",
      "0b10001000000000000000\n",
      "0b1000000010000\n",
      "0b100000001000000000\n",
      "0b100001000000000000000\n",
      "0b100000100000\n",
      "0b1000100000000000000000\n",
      "0b10000010000000000\n",
      "0b1000000001000000000000\n",
      "0b1100000000\n",
      "0b100000001\n",
      "0b100000010\n",
      "0b10100000000\n",
      "0b100000100\n",
      "0b100100000000\n",
      "0b1000100000000\n",
      "0b10000100000000\n",
      "0b100001000\n",
      "0b100000100000000\n",
      "0b1000000100000000\n",
      "0b10000000000010\n",
      "0b10000000100000000\n",
      "0b100000000100000000\n",
      "0b1000000000100000000\n",
      "0b10000000000100000000\n",
      "0b100010000\n",
      "0b100000000000100000000\n",
      "0b1000000000000100000000\n",
      "0b10001000000000\n",
      "0b10000010000000000000\n",
      "0b10000000000100\n",
      "0b100000010000000000000\n",
      "0b1000000010000000000000\n",
      "0b1000100000000000000\n",
      "0b1000001000000000000000\n",
      "0b110000000000000\n",
      "0b100100000\n",
      "0b10010000000000\n",
      "0b1000000100000\n",
      "0b1001000000000000000\n",
      "0b100000010000000000\n",
      "0b10000000001000\n",
      "0b1010000000000000\n",
      "0b100001000000000000\n",
      "0b1000000000000000010\n",
      "0b11000000000000000000\n",
      "0b101000000\n",
      "0b10100000000000\n",
      "0b100000001000000\n",
      "0b1000000001000000\n",
      "0b10000000001000000\n",
      "0b100000000001000000\n",
      "0b1000000000001000000\n",
      "0b10000000000001000000\n",
      "0b100000000000001000000\n",
      "0b1000000000000001000000\n",
      "0b10000000100000000000\n",
      "0b100000000100000000000\n",
      "0b1000000000100000000000\n",
      "0b10010000000000000\n",
      "0b1010000000000000000\n",
      "0b101000000000000000000\n",
      "0b10000000010000\n",
      "0b1000000001000000000\n",
      "0b100100000000000\n",
      "0b1001000000000000000000\n",
      "0b1000100000000000\n",
      "0b110000000\n",
      "0b11000000000000\n",
      "0b1100000000000000000\n",
      "0b10000100000000000\n",
      "0b1000001000000000000\n",
      "0b110000000000000000000\n",
      "0b100010000000000000\n",
      "0b101000000000\n",
      "0b1000010000000000000000\n",
      "0b10000000100000\n",
      "0b1000000010000000000\n",
      "0b100000100000000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "twoDif =set() # 计算仅仅两个不同时的二进制集合\n",
    "length = len(defaultSL)\n",
    "for i in range(length):\n",
    "    for j in range(length):\n",
    "        if i != j:\n",
    "            t=0\n",
    "#             print(i,j)\n",
    "            t = 1<<(i) \n",
    "            t+=1<<(j)\n",
    "#             print(bin(t))\n",
    "            twoDif.add(t)\n",
    "for i in twoDif:\n",
    "    print(bin(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "iter is  2\n",
      "gen coast:0.00000000s\n",
      "120\n",
      "scan coast:0.09599853s\n",
      "iter is  3\n",
      "gen coast:0.00000000s\n",
      "529\n",
      "scan coast:0.28920817s\n",
      "iter is  4\n",
      "gen coast:0.06396151s\n",
      "1506\n",
      "scan coast:0.79700947s\n",
      "iter is  5\n",
      "gen coast:0.32088709s\n",
      "2911\n",
      "scan coast:1.43136430s\n",
      "iter is  6\n",
      "gen coast:0.71263957s\n",
      "3914\n",
      "scan coast:1.54763031s\n",
      "iter is  7\n",
      "gen coast:0.80394101s\n",
      "3675\n",
      "scan coast:1.46617723s\n",
      "iter is  8\n",
      "gen coast:0.44769478s\n",
      "2369\n",
      "scan coast:0.94741082s\n",
      "iter is  9\n",
      "gen coast:0.07991552s\n",
      "1001\n",
      "scan coast:0.45747447s\n",
      "iter is  10\n",
      "gen coast:0.00794983s\n",
      "250\n",
      "scan coast:0.10396671s\n",
      "iter is  11\n",
      "gen coast:0.00000000s\n",
      "28\n",
      "scan coast:0.00796056s\n",
      "iter is  12\n",
      "gen coast:0.00000000s\n",
      "0\n",
      "scan coast:0.00000000s\n",
      "total coast:9.60652113s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "\n",
    "L = apriori(Items, 0.3)  # 选择频繁项集\n",
    "# print(u\"频繁项集L：\")\n",
    "# for li in L:\n",
    "#     for i in li:\n",
    "#         i.show()\n",
    "print(f'total coast:{time.time() - t:.8f}s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里对比原生的Apriori算法，可以看出答案是一样的。正确\n",
    "\n",
    "### 关于运行速度对比\n",
    "- 如果采用原生的教学数据集看不出太大差别，因为数据集很小。\n",
    "- 但是如果采用稍微正常一点的不是很稠密的数据集，不难看出。随着最小支持度的减小，二者的差别越来越大\n",
    "- 这里的数据集采用mushroom\n",
    "\n",
    "### 关于运行效率分析\n",
    "这里的数据集特征，\n",
    "- 总计8123条数据,23个特征\n",
    "- 如果数据条数足够大的时候，时间主要集中在gen上，这时候二进制编码的效率就上来了\n",
    "- 当特征太少的时候，二者跑不出太大区别，因为基本都是单项集scan和查找候选集都没有太大差别\n",
    "\n",
    "### 结果\n",
    "\n",
    "当数据集为500,23特征，支持度为0.3\n",
    "- 经典Apriori：21.2s\n",
    "- 二进制Apriori：5.8s\n",
    "\n",
    "3000,23特征，支持度为0.3\n",
    "- 经典Apriori：分钟了\n",
    "- 二进制Apriori：30.2s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "107\n",
      "401\n",
      "946\n",
      "1484\n",
      "1582\n",
      "1142\n",
      "541\n",
      "156\n",
      "23\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in L:\n",
    "    print(len(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}